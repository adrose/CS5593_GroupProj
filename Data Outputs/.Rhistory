# Libraries used
library(shiny)
library(shinydashboard)
library(ggplot2)
library(reshape)
library(wordcloud)
# Defining work environments
dashboard_location <- getwd()
base_location <- dirname(getwd())
data_location <- paste0(base_location, "/Data Outputs")
algorithms_location <- paste0(base_location, "/Algorithms")
svr_location <- paste0(base_location, "/Algorithms/SVR")
# Libraries used
library(shiny)
library(shinydashboard)
library(ggplot2)
library(reshape)
library(wordcloud)
# Defining work environments
dashboard_location <- getwd()
base_location <- dirname(getwd())
data_location <- paste0(base_location, "/Data Outputs")
algorithms_location <- paste0(base_location, "/Algorithms")
svr_location <- paste0(base_location, "/Algorithms/SVR")
# Loading data
setwd(data_location)
data <- read.csv("combined_processed_data.csv", check.names = FALSE)
# Processing data
colnames(data)[1] <- "Date"
data$Date <- as.Date(data$Date, format = "%m/%d/%y")
# Source Algorithms (i.e. cross validation)
source('rolling_cross_validation.R')
# Source Algorithms (i.e. cross validation)
setwd(algorithms_location)
source('rolling_cross_validation.R')
body <- dashboardBody(
tabItems(
tabItem(tabName = "EDA")
),
# Row 1
fluidRow(
box(title = "Confirmed cases over time", solidHeader = TRUE,
collapsible = TRUE, status = "primary",
plotOutput("covid_timeline", height = 300)),
box(title = "Inputs", solidHeader = TRUE,
collapsible = TRUE, status = "primary", height = 362,
helpText("This plot represents the daily confirmed",
"COVID-19 cases in each state plus the District of Columbia.",
"You may plot up to 5 states at the same time. To delete",
"a selected state, click on it and press 'backspace' on your",
"keyboard.",
"In addition, you can also modify the timeline between two",
"particular dates from 04/13/2020 to 09/19/2020 to investigate",
"changes over time"),
selectizeInput("timeline_state", "Choose up to five states:",
c("Alabama", "Alaska", "Arizona", "Arkansas", "California", "Colorado",
"Connecticut", "Delaware", "District of Columbia", "Florida", "Georgia",
"Hawaii", "Idaho", "Illinois", "Indiana", "Iowa", "Kansas", "Kentucky",
"Louisiana", "Maine", "Maryland", "Massachusetts", "Michigan", "Minnesota",
"Mississippi", "Missouri", "Montana", "Nebraska", "Nevada", "New Hampshire",
"New Jersey", "New Mexico", "New York", "North Carolina", "North Dakota",
"Ohio", "Oklahoma", "Oregon", "Pennsylvania", "Rhode Island", "South Carolina",
"South Dakota", "Tennessee", "Texas", "Utah", "Vermont", "Virginia", "Washington",
"West Virginia", "Wisconsin", "Wyoming"),
options = list(maxItems = 5),
selected = "Alabama"),
dateRangeInput("timeline_date", "Choose date range:",
start = "2020-04-13",
end = "2020-09-19",
min = "2020-04-13",
max = "2020-09-19",
format = "yy/mm/dd",
separator = " - "))
),
fluidRow(
box(title = "Twitter words frequency over time", solidHeader = TRUE,
collapsible = TRUE, status = "success",
plotOutput("wordmap", height = 500)),
box(title = "Inputs", solidHeader = TRUE,
collapsible = TRUE, status = "success", height = 560,
helpText("This plot represents the most commonly used words in",
"COVID-19 related tweets averaged over time.",
"You may plot up to 411 words at the same time. They are drawn",
"in no particular order from the dataset.",
"In addition, you can also modify the timeline between two",
"particular dates from 04/13/2020 to 09/19/2020 to investigate",
"changes over time.",
"Lastly, you can remove COVID-19 references from the wordmap",
"(e.g. covid, covid19, coronavirus) from the wordmap."),
numericInput("numb_words", "Number of words to populate map",
min = 10, max = 411, value = 10),
dateRangeInput("wordmap_date", "Choose date range:",
start = "2020-04-13",
end = "2020-09-19",
min = "2020-04-13",
max = "2020-09-19",
format = "yy/mm/dd",
separator = " - "),
checkboxInput("remove_covid", "Remove COVID-19 references", value = FALSE))
)
)
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
52:463
463 - 52
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
runApp('F:/2. PhD/2. Github/CS5593_GroupProj/Dashboard')
### Defining working directory ----
script_location <- getwd()
root_folder <- dirname(script_location)
data_folder <- paste0(root_folder, '/Data Outputs')
svr_location <- paste0(root_folder, '/Algorithms/SVR/')
rf_location <- paste0(script_location, "/RandomForest")
knn_location <- script_location
### Sourcing SVR auxiliary R files ----
setwd(svr_location)
source('svr_functions.R')
### Sourcing Cross validation file ----
setwd(script_location)
source('rolling_cross_validation.R')
### Sourcing RF R files ----
setwd(rf_location)
source('randomForestFunctions.R')
# Libraries for RF
library(foreach)
library(doParallel)
### Loading data ----
setwd(data_folder)
data <- read.csv("combined_processed_data.csv")
# Extracting Y (includes all states + districts)
# Y_all = data[, 1:52]
# But for now, let's just test one single Y
Y = data[, 2]
# Generally, we would use all predictors:
# X = data[, 53:ncol(data)]
# But for now, let's test just a couple
X = data[, 54:ncol(data)]
# Scaling the data
X = scale(X)
Y = scale(Y)
## Defining the cross validation parameters
# start_size = 40
# K = 10
start_size = 100
K = 30
model = "SVR"
### Defining hyperparameter grid
## SVR
# degree of polynomials (No more than 2)
p1 = seq(from = 1, to = 2, by = 1)
# type of kernel
ke = c("polynomial", "linear")
# Cost
C = seq(from = 1, to = 5, by = 1)
# e (No more than 0.5)
e = seq(from = 0.1, to = 0.5, by = 0.1)
# Creating the grid
hyperparameters <- expand.grid(poly_deg = p1, ke = ke,
C = C, e = e)
### Running the function
wew <- roll_cross_validation(X, Y, start_size, K, model, hyperparameters)
Y = as.data.frame(data[, 2])
colnames(Y) <- colnames(data[2])
X = data[, 54:ncol(data)]
n_trees <- seq(from = 200, to = 250, by = 5)
feature_frac <- seq(from = 0.5, to = 0.75, by = 0.05)
min_node <- seq(from = 1, to = 3, by = 1)
hyperparameters <- expand.grid(n_trees = n_trees, feature_frac = feature_frac,
min_node = min_node)
wew <- roll_cross_validation(X, Y, start_size = 40, K = 10, model = "RF", hyperparameters)
wew <- roll_cross_validation(X, Y, start_size = 40, K = 10, model = "RF", hyperparameters)
Y = as.data.frame(data[, 2])
colnames(Y) <- colnames(data[2])
X = data[, 54:ncol(data)]
n_trees <- seq(from = 200, to = 250, by = 5)
feature_frac <- seq(from = 0.5, to = 0.75, by = 0.05)
min_node <- seq(from = 1, to = 3, by = 1)
hyperparameters <- expand.grid(n_trees = n_trees, feature_frac = feature_frac,
min_node = min_node)
View(reg_tree_imp)
wew <- roll_cross_validation(X, Y, start_size = 40, K = 10, model = "RF", hyperparameters)
### Sourcing Cross validation file ----
setwd(script_location)
source('rolling_cross_validation.R')
### Defining working directory ----
script_location <- getwd()
root_folder <- dirname(script_location)
data_folder <- paste0(root_folder, '/Data Outputs')
svr_location <- paste0(root_folder, '/Algorithms/SVR/')
rf_location <- paste0(script_location, "/RandomForest")
knn_location <- script_location
### Sourcing SVR auxiliary R files ----
setwd(svr_location)
source('svr_functions.R')
### Sourcing Cross validation file ----
setwd(script_location)
source('rolling_cross_validation.R')
### Sourcing RF R files ----
setwd(rf_location)
source('randomForestFunctions.R')
# Libraries for RF
library(foreach)
library(doParallel)
### Sourcing KNN R files ----
# No files
# Libraries for KNN
library(tidyverse)
library(caret)
### Loading data ----
setwd(data_folder)
data <- read.csv("combined_processed_data.csv")
Y = as.data.frame(data[, 3])
colnames(Y) <- colnames(data[3])
X = data[, 54:ncol(data)]
n_trees <- seq(from = 10, to = 20, by = 5)
feature_frac <- seq(from = 0.5, to = 0.75, by = 0.05)
min_node <- seq(from = 2, to = 3, by = 1)
hyperparameters <- head(expand.grid(n_trees = n_trees, feature_frac = feature_frac,
min_node = min_node), 5)
wew <- roll_cross_validation(X, Y, start_size = 120, K = 40, model = "RF", hyperparameters)
