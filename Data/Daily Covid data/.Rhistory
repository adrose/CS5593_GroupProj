rm(list = ls())
# Defining environment where data is located
setwd("./Data/Daily Covid data")
# Collecting .csv filenames from folder
filenames = list.files(pattern="*.csv")
# Vectors of states we are interested in
states <- c("Alabama", "Alaska", "Arizona", "Arkansas", "California", "Colorado",
"Connecticut", "Delaware", "District of Columbia", "Florida", "Georgia",
"Hawaii", "Idaho", "Illinois", "Indiana", "Iowa", "Kansas", "Kentucky",
"Louisiana", "Maine", "Maryland", "Massachusetts", "Michigan", "Minnesota",
"Mississippi", "Missouri", "Montana", "Nebraska", "Nevada", "New Hampshire",
"New Jersey", "New Mexico", "New York", "North Carolina", "North Dakota",
"Ohio", "Oklahoma", "Oregon", "Pennsylvania", "Rhode Island", "South Carolina",
"South Dakota", "Tennessee", "Texas", "Utah", "Vermont", "Virginia", "Washington",
"West Virginia", "Wisconsin", "Wyoming")
# Defining the date range we are interested in
dates <- seq.Date(from = as.Date("04/12/2020", format = "%m/%d/%Y"),
to = as.Date("09/19/2020", format = "%m/%d/%Y"), by = "day")
# Creating the dataframe in which the confirmed cases will be stored
covid_data <- as.data.frame(matrix(0, nrow = length(states), ncol = length(dates)))
rownames(covid_data) <- states
colnames(covid_data) <- dates
View(covid_data)
# Libraries used
library(dplyr)
library(data.table) # For transpose function
library(tm) # For list of stopwords
# Defining location of data
setwd("./Data")
# ----------------------------------------------------
# ----------------------------------------------------
### Loading data
## Data from 03/23 to 09/19
tweet_data <- read.csv("includeCount_49_dateLower_2020-03-22_dateUpper_2020-09-22TwitterCount.csv")
# Last row is full of NA's
tweet_data <- tweet_data[-182, ]
# ----------------------------------------------------
# ----------------------------------------------------
#
# Pre-processing of Twitter data
#
# ----------------------------------------------------
# ----------------------------------------------------
## Step 1: handling missing values ----
# How many missing values are there?
sum(is.na(tweet_data))
# How many attributes have NO missing values?
sum(colSums(is.na(tweet_data)) == 0)
# Removing attributes with missing values
index_no_na <- colSums(is.na(tweet_data)) == 0
tweet_data_no_na <- tweet_data[, index_no_na]
## Step 2: Removing stopwords ----
# Creating list of words to possibly remove
remove_list <- list()
count = 1
for(i in 2:ncol(tweet_data_no_na)){
if((colnames(tweet_data_no_na)[i] %in% stopwords("en")) == TRUE){
remove_list[[count]] <- i
count = count + 1
} else if((colnames(tweet_data_no_na)[i] %in% stopwords("german")) == TRUE){
remove_list[[count]] <- i
count = count + 1
} else if((colnames(tweet_data_no_na)[i] %in% stopwords("french")) == TRUE){
remove_list[[count]] <- i
count = count + 1
} else if((colnames(tweet_data_no_na)[i] %in% stopwords("italian")) == TRUE){
remove_list[[count]] <- i
count = count + 1
} else if((colnames(tweet_data_no_na)[i] %in% stopwords("spanish")) == TRUE){
remove_list[[count]] <- i
count = count + 1
}
}
# The words to be removed are:
word_index <- unlist(remove_list, use.names = FALSE)
colnames(tweet_data_no_na[, word_index])
# Now removing them:
tweet_data_no_na_stopwords <- tweet_data_no_na[, -word_index]
## Step 3: Removing non-ASCII letters ----
non_ascii <- colnames(tweet_data_no_na_stopwords)[which(grepl("[^\x01-\x7F]+", colnames(tweet_data_no_na_stopwords)))]
remove_list <- list()
count = 1
for(i in 2:ncol(tweet_data_no_na_stopwords)){
if((colnames(tweet_data_no_na_stopwords)[i] %in% non_ascii) == TRUE){
remove_list[[count]] <- i
count = count + 1
}
}
# The words to be removed are:
word_index <- unlist(remove_list, use.names = FALSE)
colnames(tweet_data_no_na_stopwords[, word_index])
# Now removing them:
tweet_data_no_na_stopwords_nascii <- tweet_data_no_na_stopwords[, -word_index]
## Step 4: Remove words with length less than or equal to 2 ----
remove_list <- list()
count = 1
for(i in 2:ncol(tweet_data_no_na_stopwords_nascii)){
if((nchar(colnames(tweet_data_no_na_stopwords_nascii)[i])) <= 2){
remove_list[[count]] <- i
count = count + 1
}
}
# The words to be removed are:
word_index <- unlist(remove_list, use.names = FALSE)
colnames(tweet_data_no_na_stopwords_nascii[, word_index])
# Now removing them
tweet_data_no_na_stopwords_nascii_less2 <- tweet_data_no_na_stopwords_nascii[, -word_index]
# ------------------------------------------------------------------------------------------------------
# ------------------------------------------------------------------------------------------------------
## Step 5: Correlations ----
# Removing date
tweet_data_no_na_stopwords_nascii_less2 <- tweet_data_no_na_stopwords_nascii_less2[, -1]
# Correlations
corr_mat <- round(cor(tweet_data_no_na_stopwords_nascii_less2), 2)
# Finding highly correlated variables
corr_index <- findCorrelation(corr_mat, cutoff = .90, verbose = FALSE)
corr_index
tweet_data_no_na_stopwords_nascii_less2_nocorr <- tweet_data_no_na_stopwords_nascii_less2[, -corr_index]
# Final list of words:
colnames(tweet_data_no_na_stopwords_nascii_less2_nocorr)
# Final tweet data set
tweet_data_final <- tweet_data_no_na_stopwords_nascii_less2_nocorr
# ----------------------------------------------------
# ----------------------------------------------------
#
# Exporting the data
#
# ----------------------------------------------------
# ----------------------------------------------------
# Exporting the data
write.csv(tweet_data_final, file = "processed_tweet_data.csv")
